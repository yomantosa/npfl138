title: NPFL138, Lecture 7
class: title, langtech, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }

# Recurrent Neural Networks

## Milan Straka

### April 1, 2025

---
section: RNN
class: section
# Recurrent Neural Networks

---
# Recurrent Neural Networks

## Single RNN cell

![w=17%,h=center](rnn_cell.svgz)

~~~

## Unrolled RNN cells

![w=60%,h=center](rnn_cell_unrolled.svgz)

---
# Basic RNN Cell

![w=100%,h=center,mw=50%](rnn_cell_basic.svgz)![w=50%,h=center,mw=50%](rnn_cell_basic_as_cell.svgz)

Given an input $→x^{(t)}$ and previous state $→h^{(t-1)}$, the new state is computed as
$$→h^{(t)} = f(→h^{(t-1)}, →x^{(t)}; →θ).$$

~~~
One of the simplest possibilities (called `torch.nn.{RNN,RNNCell}` in PyTorch) is
$$→h^{(t)} = \tanh(⇉U→h^{(t-1)} + ⇉V→x^{(t)} + →b).$$

---
# Basic RNN Cell

Basic RNN cells suffer a lot from vanishing/exploding gradients (the so-called
**challenge of long-term dependencies**).

~~~
If we simplify the recurrence of states to just a linear approximation
$$→h^{(t)} ≈ ⇉U→h^{(t-1)},$$

~~~
we get $→h^{(t)} ≈ ⇉U^t→h^{(0)}$.

~~~
If $⇉U$ has an eigenvalue decomposition of $⇉U = ⇉Q ⇉Λ ⇉Q^{-1}$, we get that
$$→h^{(t)} ≈ ⇉Q ⇉Λ^t ⇉Q^{-1} →h^{(0)}.$$
The main problem is that the _same_ function is iteratively applied many times.

~~~
Several more complex RNN cell variants have been proposed, which alleviate
this issue to some degree, namely **LSTM** and **GRU**.

---
section: LSTM
class: section
# Long Short-Term Memory (LSTM)

---
# Long Short-Term Memory

Hochreiter & Schmidhuber (1997) suggested that to enforce
_constant error flow_, we would like
$$f' = →1.$$

~~~
They propose to achieve that by a _constant error carrousel_.

![w=60%,h=center](lstm_cec_idea.svgz)

~~~ ~~
They propose to achieve that by a _constant error carrousel_.

![w=60%,h=center](lstm_cec.svgz)

---
# Long Short-Term Memory

They also propose an **input** and **output** gates which control the flow
of information into and out of the carrousel (**memory cell** $→c_t$).

![w=40%,f=right](lstm_input_output_gates.svgz)

$$\begin{aligned}
  \textcolor{blue}     {→i_t} & ← σ(⇉W^i →x_t + ⇉V^i →h_{t-1} + →b^i) \\
  \textcolor{darkgreen}{→o_t} & ← σ(⇉W^o →x_t + ⇉V^o →h_{t-1} + →b^o) \\
  \textcolor{magenta}  {→c_t} & ← →c_{t-1} + →i_t ⊙ \tanh(⇉W^y →x_t + ⇉V^y →h_{t-1} + →b^y) \\
  \textcolor{red}      {→h_t} & ← →o_t ⊙ \tanh(→c_t)
\end{aligned}$$

---
# Long Short-Term Memory

Later, Gers, Schmidhuber & Cummins (1999) added a possibility to **forget**
information from memory cell $→c_t$.

![w=40%,f=right](lstm_input_output_forget_gates.svgz)

$$\begin{aligned}
  \textcolor{blue}      {→i_t} & ← σ(⇉W^i →x_t + ⇉V^i →h_{t-1} + →b^i) \\
  \textcolor{darkorange}{→f_t} & ← σ(⇉W^f →x_t + ⇉V^f →h_{t-1} + →b^f) \\
  \textcolor{darkgreen} {→o_t} & ← σ(⇉W^o →x_t + ⇉V^o →h_{t-1} + →b^o) \\
  \textcolor{magenta}   {→c_t} & ← →f_t ⊙ →c_{t-1} + →i_t ⊙ \tanh(⇉W^y →x_t + ⇉V^y →h_{t-1} + →b^y) \\
  \textcolor{red}       {→h_t} & ← →o_t ⊙ \tanh(→c_t)
\end{aligned}$$

~~~
Note that since 2015, following the paper
- R. Jozefowicz et al.: _An Empirical Exploration of Recurrent Network Architectures_

the forget gate bias $→b^f$ is usually initialized to 1, so that the forget gate is closer
to 1 and the gradients can easily flow through multiple timesteps.
~~~
(Gers et al. advocated this in the original paper already.)
~~~
(BTW, I think 3 might be even better, as $σ(1) ≈ 0.731$, $σ(3) ≈ 0.953$.)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-SimpleRNN.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-chain.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-C-line.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-focus-i.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-focus-f.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-focus-C.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-focus-o.png)

---
section: GRU
class: section
# Gated Recurrent Unit (GRU)

---
style: .katex-display { margin: .7em 0; }
# Gated Recurrent Unit

**Gated recurrent unit (GRU)** was proposed by Cho et al. (2014) as
a simplification of LSTM. The main differences are

![w=44%,f=right](gru.svgz)

- no memory cell,
- forgetting and updating tied together.

~~~
$$\begin{aligned}
  \textcolor{blue}     {→r_t} & ← σ(⇉W^r →x_t + ⇉V^r →h_{t-1} + →b^r) \\
  \textcolor{darkgreen}{→u_t} & ← σ(⇉W^u →x_t + ⇉V^u →h_{t-1} + →b^u) \\
  \textcolor{magenta}  {→ĥ_t} & ← \tanh(⇉W^h →x_t + ⇉V^h (→r_t ⊙ →h_{t-1}) + →b^h) \\
  \textcolor{red}      {→h_t} & ← →u_t ⊙ →h_{t-1} + (1 - →u_t) ⊙ →ĥ_t
\end{aligned}$$

~~~
The formulas above are the from the original paper; however, all GPU-accelerated
implementations I know of use a slight modification with reset gate applied
after the matrix multiplication:

$\displaystyle\kern1.25em \textcolor{magenta}  {→ĥ_t} ← \tanh(⇉W^h →x_t + →r_t ⊙ (⇉V^h →h_{t-1}) + →b^h),$

because it is slightly faster and it seems to have nearly identical performance.

---
# Gated Recurrent Unit
![w=100%,v=middle](LSTM3-var-GRU.png)

---
# GRU and LSTM Differences

The main differences between GRU and LSTM:
~~~
- GRU uses fewer parameters and less computation.

  - six matrices $⇉W$, $⇉V$ instead of eight
~~~
- GRU are easier to work with, because the state is just one tensor, while it is
  a pair of tensors for LSTM.
~~~
- In most tasks, LSTM and GRU give very similar results.
~~~
- However, there are some tasks, on which LSTM achieves (much) better results
  than GRU.
~~~
  - For a demonstration of difference in the expressive power of LSTM and GRU
    (caused by the coupling of the forget and update gate), see the paper
    - G. Weiss et al.: _On the Practical Computational Power of Finite Precision
      RNNs for Language Recognition_ https://arxiv.org/abs/1805.04908
~~~
  - For a difference between LSTM and GRU on a real-word task, see for example
    - T. Dozat et al.: _Deep Biaffine Attention for Neural Dependency Parsing_
      https://arxiv.org/abs/1611.01734

---
class: middle
# GRU and LSTM Comparison

![w=50%](lstm_gru_accuracies.svgz)![w=90%,mw=50%,h=center](lstm_gru_ptb.svgz)

---
# SimpleRNN, GRU, and LSTM Initialization

Recall that when we approximate $→h^{(t)} ≈ ⇉U→h^{(t-1)}$,
assuming the eigenvalue decomposition of $⇉U = ⇉Q ⇉Λ ⇉Q^{-1}$, we get
$$→h^{(t)} ≈ ⇉Q ⇉Λ^t ⇉Q^{-1} →h^{(0)}.$$

~~~
This motivated a specific initialization scheme for the $⇉U$ matrix in Keras and
TensorFlow – this so-called **recurrent kernel** (the concatenation of all the
$⇉V^i$, $⇉V^f$, $⇉V^o$, $⇉V^y$ matrices) is initialized with a randomly
generated orthogonal matrix. This **orthogonal** initialization is available as
`torch.nn.init.orthogonal_` in PyTorch, but it is not used as the default
initialization of RNNs.

~~~
Our `npfl138` module changes the RNN initialization defaults to:
- initialize the recurrent kernel using orthogonal initialization,

- initialize the non-recurrent kernel using Glorot (Xavier) initialization,
- initialize biases to zero, with the exception of forget cell bias in LSTM
  initialized to 1.

---
section: Highway
class: section
# Highway Networks

---
# Highway Networks

For input $→x$, fully connected layer computes
$$→y ← H(→x, ⇉W_H).$$

~~~
Highway networks add residual connection with gating:
$$→y ← H(→x, ⇉W_H) ⊙ T(→x, ⇉W_T) + →x ⊙ (1 - T(→x, ⇉W_T)).$$

~~~
Usually, the gating is defined as
$$T(→x, ⇉W_T) ← σ(⇉W_T →x + →b_T).$$

~~~
Note that the resulting update is very similar to a GRU cell with $→h_t$ removed; for a
fully connected layer $H(→x, ⇉W_H) = \tanh(⇉W_H →x + →b_H)$ it is exactly it,
apart from copying $→x$ instead of $→h_{t-1}$.

~~~
Analogously to LSTM, the transform gate bias $→b_T$ should be initialized to
a negative number.

---
# Highway Networks on MNIST

![w=100%](highway_training.svgz)

---
# Highway Networks

![w=95%,h=center](highway_leisoning.svgz)

---
section: RNNRegularization
class: section
# Regularizing RNNs

---
# Regularizing RNNs

## Dropout

- Using dropout on hidden states interferes with long-term dependencies.

~~~

- However, using dropout on the inputs and outputs works well and is used
frequently.
~~~
  - In case residual connections are present, the output dropout needs to be
    applied before adding the residual connection.
~~~
  - In PyTorch, `torch.nn.{RNN,LSTM,GRU}` has a parameter `dropout`, which adds
    a dropout layer with given dropout probability on the output of _all but the
    last RNN layers_, i.e., on the places that you cannot place it manually.
~~~
    - _However, using a multi-layer `torch.nn.{RNN,LSTM,GRU}` module does not
      use residual connections, so personally I never use the multi-layer variant._

~~~
- Several techniques were designed to allow using dropout on hidden states.
    - Variational Dropout
    - Recurrent Dropout
    - Zoneout

---
# Regularizing RNNs

## Variational Dropout

![w=75%,h=center](variational_rnn.svgz)

~~~
To implement variational dropout on inputs, the same dropout mask must be used
for all time steps (`torch.nn.Dropout1d` in PyTorch; using `noise_shape` in
Keras).

~~~
In practice, the variational dropout on the hidden states is not frequently
used, because it is not supported by GPU-accelerated algorithms.

---
# Regularizing RNNs

## Recurrent Dropout

Dropout only candidate states (i.e., values added to the memory cell in LSTM and
previous state in GRU), independently in every time-step.

~~~
## Zoneout

Randomly preserve hidden activations instead of dropping them.

~~~
## Batch Normalization

![w=42%,f=right](recurrent_batch_normalization.svgz)

Very fragile and sensitive to proper initialization – there were papers with
negative results (_Dario Amodei et al, 2015: Deep Speech 2_ or _Cesar Laurent et al,
2016: Batch Normalized Recurrent Neural Networks_) until people managed to make
it work (_Tim Cooijmans et al, 2016: Recurrent Batch Normalization_;
specifically, initializing $γ=0.1$ did the trick).

---
section: LayerNorm
class: section
# Layer Normalization

---
# Regularizing RNNs

## Batch Normalization

Neuron value is normalized across the minibatch, and in case of CNN also across
all positions.

~~~
## Layer Normalization

Neuron value is normalized across the layer.

~~~
![w=100%](../06/normalizations.svgz)

---
# Layer Normalization

Consider a hidden value $→x ∈ ℝ^D$. Layer normalization (both during training and
during inference) is performed as follows.

<div class="algorithm">

**Inputs**: An example $→x ∈ ℝ^D$, $ε ∈ ℝ$ with default value 0.001<br>
**Parameters**: $→β ∈ ℝ^D$ initialized to $→0$, $→γ ∈ ℝ^D$ initialized to $→1$<br>
**Outputs**: Normalized example $→y$

~~~
- $μ ← \frac{1}{D} ∑_{i = 1}^D x_i$

~~~
- $σ^2 ← \frac{1}{D} ∑_{i = 1}^D (x_i - μ)^2$
~~~
- $→x̂ ← (→x - μ) / \sqrt{σ^2 + ε}$
~~~
- $→y ← →γ ⊙ →x̂ + →β$
</div>

~~~
When applying layer normalization after convolutions on an image, we want it to
be positional invariant (the same as with batch normalization); therefore, we
have parameters only for channels, i.e., $→β ∈ ℝ^C$ and $→γ ∈ ℝ^C$, but we compute
$μ$ and $σ^2$ over the whole image.

---
# Regularizing RNNs

## Layer Normalization

Much more stable than batch normalization for RNN regularization.

![w=70%,h=center](layer_norm.svgz)

~~~
![w=85%,h=center](layer_norm_properties.svgz)

---
# Layer Normalization

In an important recent architecture (the Transformer architecture), many fully
connected layers are used, with a residual connection and a layer normalization.

![w=85%,h=center](layer_norm_residual.svgz)

~~~
This could be considered a ResNet-like alternative of residual connections for
fully-connected layers (better than highway networks).
~~~
Note the architecture can be interpreted as a variant of a mobile inverted
bottleneck $1×1$ convolution block.

---
# Group Normalization

Group Normalization is analogous to Layer normalization, but the channels are
normalized in groups (by default, $G=32$).

![w=40%,h=center](../06/normalizations.svgz)

~~~
![w=40%,h=center](../06/group_norm.svgz)

---
# Group Normalization

![w=78%,h=center](../06/group_norm_vs_batch_norm.svgz)

---
# Group Normalization

![w=65%,h=center](../06/group_norm_coco.svgz)

---
section: RNNArchitectures
class: section
# RNN Architectures and Tasks

---
# Basic RNN Architectures and Tasks

## Sequence Element Representation

Create output for individual elements, for example for classification of the
individual elements.

![w=70%,h=center](rnn_cell_unrolled.svgz)

~~~
## Sequence Representation

Generate a single output for the whole sequence (either the last output or the
last state).

---
# Basic RNN Architectures and Tasks

## Sequence Prediction

During training, predict next sequence element.

![w=75%,h=center](sequence_prediction_training.svgz)

~~~
During inference, use predicted elements as further inputs.

![w=75%,h=center](sequence_prediction_inference.svgz)

---
# Multilayer RNNs

We might stack several layers of recurrent neural networks. Usually using two or
three layers gives better results than just one.

![w=75%,h=center](multilayer_rnn.svgz)

---
# Multilayer RNNs

In case of multiple layers, residual connections usually improve results.
Because dimensionality has to be the same, they are usually applied from the
second layer.

![w=75%,h=center](multilayer_rnn_residual.svgz)

---
# Bidirectional RNN

To consider both the left and right contexts, a **bidirectional** RNN can be used,
which consists of parallel application of a **forward** RNN and a **backward** RNN.

![w=80%,h=center](bidirectional_rnn.svgz)

~~~
The outputs of both directions can be either **added** or **concatenated**. Even
if adding them does not seem very intuitive, it does not increase
dimensionality and therefore allows residual connections to be used in case
of multilayer bidirectional RNN.

---
section: WE
class: section
# Word Embeddings

---
# Word Embeddings

We might represent **words** using one-hot encoding, considering all words to be
independent of each other.

~~~
However, words are not independent – some are more similar than others.

~~~
Ideally, we would like some kind of similarity in the space of the word
representations.

~~~
## Distributed Representation
The idea behind distributed representation is that objects can
be represented using a set of common underlying factors.

~~~
We therefore represent words as fixed-size **embeddings** into $ℝ^d$ space,
with the vector elements playing role of the common underlying factors.

~~~
These embeddings are initialized randomly and trained together with the rest of
the network.

---
# Word Embeddings

The word embedding layer is in fact just a fully connected layer on top of
one-hot encoding. However, it is not implemented in that way.

~~~
Instead, the so-called **embedding** layer is used, which is much more efficient.
When a matrix is multiplied by an one-hot encoded vector (all but one zeros
and exactly one 1), the row corresponding to that 1 is selected, so the
embedding layer can be implemented only as a simple lookup.

~~~
In PyTorch, the embedding layer is available as
```python
torch.nn.Embedding(input_dim, output_dim)
```

---
# Word Embeddings

Even if the embedding layer is just a fully connected layer on top of one-hot
encoding, it is important that this layer is _shared_ across
the whole network.

~~~
![w=37.5%](words_onehot.svgz)
~~~
![w=60.5%](words_embeddings.svgz)

---
section: CLE
class: section
# Character-Level Word Embeddings

---
section: CLE
# Word Embeddings for Unknown Words

![w=42%,f=right](cle_rnn.svgz)

## Recurrent Character-level WEs

In order to handle words not seen during training, we could find a way
to generate a representation from the word **characters**.

~~~
A possible way to compose the representation from individual characters
is to use RNNs – we embed _characters_ to get character representation,
and then use an RNN to produce the representation of a whole _sequence of
characters_.

~~~
Usually, both forward and backward directions are used, and the resulting
representations are concatenated/added.

---
# Examples of Recurrent Character-level WEs

![w=80%,h=center](cle_rnn_examples.svgz)

---
# Word Embeddings for Unknown Words

## Convolutional Character-level WEs

![w=32%,f=right](cle_cnn.png)

Alternatively, 1D convolutions might be used.

~~~
Assume we use a 1D convolution with kernel size 3. It produces a representation
for every input word trigram, but we need a representation of the whole word.
To that end, we use _global max-pooling_ – using it has an interpretable
meaning, where the kernel is a _pattern_ and the activation after the maximum
is a level of a highest match of the pattern anywhere in the word.

~~~
Kernels of varying sizes are usually used (because it makes sense to have
patterns for unigrams, bigrams, trigrams, …) – for example, 25 filters for every
kernel size $(1, 2, 3, 4, 5)$ might be used.

~~~
Lastly, authors employed a highway layer after the convolutions, improving
the results (compared to not using any layer or using a fully connected one).

---
# Examples of Convolutional Character-level WEs

![w=100%](cle_cnn_examples.svgz)

---
# Character-level WE Implementation

## Training

- Generate unique words per batch.

~~~
- Process the unique words in the batch.

~~~
- Copy the resulting embeddings suitably in the batch.

~~~
## Inference

- We can cache character-level word embeddings during inference.

---
# NLP Processing with CLEs

![w=100%,v=middle](cle_rnn_gru.png)

